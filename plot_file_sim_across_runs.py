"""
    This file contains the code to plot the similarity scores of the same file
    across different runs. Each file represents a solution to a prompt with
    optimization sentance, if applicable, of a specific LLM. The similarity
    score is calculated using pycode-similar. The data that this program uses is
    generated by the framework.

"""

import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import subprocess
import re

problem_names = ['Sort_List', 'Median_of_Two_Sorted_Arrays', 'Assign_Cookies']
optimizations = ['energy', 'library_functions', 'for_loop']

def calculate_similarity_between_files(reference_file_name, candidate_file_name):
    """
        Calculate similarity between two files using pycode-similar and return the percentage.

        Args:
            reference_file_name (str): Path to the reference file name.
            candidate_file_name (str): Path to the candidata file name.
        Returns:
            float: Similarity percentage between the two files.
    """

    result = subprocess.run(
        ['pycode_similar', reference_file_name, candidate_file_name],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True
    )

    if result.returncode != 0:
        print("Error running pycode_similar:", result.stderr)
        return -1

    output = result.stdout

    match = re.search(r'(\d+\.\d+) %', output)
    if match:
        percentage = float(match.group(1))
        return percentage
    else:
        print("Similarity percentage not found in the output.")
        return -1

def extract_llm_name(file_name):
    """
        Extract the LLM name from the file name.

        Args:
            file_name (str): Name of the file.
        Returns:
            str: Name of the LLM.
    """
    match = re.search(r'_(CodeLlama-\d+b(?:-Python|-Instruct)?|deepseek-coder-\d+b(?:-instruct|-base)?)\.py', file_name)
    return match.group(1) if match else ''

def plot(data, x_label_values, y_label_values, x_label, y_label, plot_title, right_shift=0.5):
    """

    """
    plt.figure(figsize=(16, 9))
    sns.heatmap(data, cmap='viridis', annot=True, fmt=".2f")

    plt.xticks(ticks=np.arange(data.shape[1]) + 0.5, labels=x_label_values, ha='center')
    plt.yticks(ticks=np.arange(data.shape[0]) + 0.5, labels=y_label_values, rotation=0, va='center')

    plt.tick_params(axis='x', which='both', bottom=True, top=False)
    plt.tick_params(axis='y', which='both', left=True, right=False)

    plt.xlabel(x_label, position=(0.5, 5))
    plt.ylabel(y_label, position=(0, 0.5))

    plt.subplots_adjust(left=0.0 + right_shift)
    plt.title(plot_title)


    if not os.path.exists('plots'):
        os.makedirs('plots')

    plt.savefig(f'plots/'+ plot_title.replace(" ", "_"), bbox_inches='tight')

def extract_optimization_name_from_file_name(file_name):
    match = re.search(r'(energy|library_functions|for_loop)', file_name)
    return match.group(1) if match else 'base'


def sim_file_across_runs(problem_name):
    """
        Function to calculate the similarity between the optimization and base
        version of a prompt for the same LLM across different runs.
    """
    output_dirs = ['output_run0', 'output_run1', 'output_run2',
                              'output_run3', 'output_run4', 'output_run5',
                              'output_run6', 'output_run7', 'output_run8',
                              'output_run9']

    list_of_file_names = []

    for file_name in os.listdir(output_dirs[0]):
        if file_name.endswith('.py'):
            list_of_file_names.append(file_name)


    list_of_file_names = sorted(list_of_file_names, key=extract_llm_name)

    y_labels = []
    count = 0
    for file_name in os.listdir(output_dirs[0]):
        if problem_name in file_name:
            count += 1

    similarities = np.zeros((count, len(output_dirs)))

    index_y = 0
    for file_name in list_of_file_names:
        index_x = 0
        if problem_name in file_name:
            reference_file_name = output_dirs[0] + '/' + file_name
            for output_dir in output_dirs:
                candidate_file_name = output_dir + '/' + file_name
                similarity = calculate_similarity_between_files(reference_file_name, candidate_file_name)
                similarities[index_y, index_x] = similarity
                index_x += 1

            llm_name = extract_llm_name(file_name)
            optimizations = extract_optimization_name_from_file_name(file_name).replace('_', ' ')
            y_labels.append(f'{llm_name} {optimizations}')
            index_y += 1

    x_labels = [f'Run{i}' for i in range(len(output_dirs))]

    title = 'Code similarity between the ' + code_problem_name.replace('_', ' ') + ' solutions in different runs. Run 0 is the reference run.'
    x_label = 'Run number'
    y_label = 'Name of the code solution, optimaztion name (if applicable) and LLM name'
    plot(similarities, x_labels, y_labels, x_label, y_label, title, right_shift=0.5)


if __name__ == '__main__':
    for code_problem_name in problem_names:
        sim_file_across_runs(code_problem_name)
